#!/usr/bin/env python3
# uco_v10_21.py
# Python implementation (single-file) of UCO - "V10.21 - Proactively Adaptive, Auditable & Robust"
# Target runtime: python>=3.12, pytorch==2.8.0 (code uses PyTorch API; falls back gracefully)
# Author: generated (adapt and extend for production)
# Seeds: [42, 123, 999, 2025, 7]

import os
import sys
import time
import json
import hashlib
import random
import argparse
import logging
from dataclasses import dataclass, field
from typing import Any, Dict, Tuple, Callable, Optional, List

# Numeric libs
import numpy as np

# Try import PyTorch, otherwise minimal fallback (script still runs simulation without torch ops)
try:
    import torch
    has_torch = True
    torch.set_float32_matmul_precision('high')
    # use deterministic algorithms as spec
    try:
        torch.use_deterministic_algorithms(True)
    except Exception:
        # older/newer torch may differ
        pass
except Exception:
    has_torch = False
    torch = None

# Optional helpful libs
try:
    from sklearn.isotonic import IsotonicRegression
    from sklearn.isotonic import _isotonic_regression
    has_isotonic = True
except Exception:
    has_isotonic = False

try:
    from prometheus_client import start_http_server, Gauge
    prometheus_available = True
except Exception:
    prometheus_available = False

# ---------------------------
# Config & Manifest
# ---------------------------
SEEDS = [42, 123, 999, 2025, 7]
DEFAULT_SEED = SEEDS[0]
CHECKPOINT_DIR = "./checkpoints"
os.makedirs(CHECKPOINT_DIR, exist_ok=True)

CONFIG = {
    "version": "V10.21 - Proactively Adaptive, Auditable & Robust",
    "min_N": 8000,
    "ewma_alpha": 0.1,
    "ewma_window": 100,
    "ood_shift_threshold_pct": 10.0,
    "drift_ece_delta_retrain": 0.001,
    "retrain_min_samples": 10000,
    "cooldown_seconds": 6 * 3600,   # 6h
    "hysteresis_windows": 3,
    "checkpoint_interval_sec": 30 * 60,  # 30 min
    "prune_delta_ece": 0.0005,
    "prune_delta_brier": 0.0003,
    "pgd_adv_pct": 160,
    "ars_fdp_eps": 1.0,
    "target_brier": 0.004,
    "target_ece": 0.001
}

# Logging
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s | %(levelname)s | %(message)s",
    handlers=[logging.StreamHandler(sys.stdout)]
)
logger = logging.getLogger("UCO")

# Prometheus metrics (optional)
if prometheus_available:
    GAUGE_ECE = Gauge("uco_ece_global", "Global ECE (UCO)")
    GAUGE_BRIER = Gauge("uco_brier_global", "Global Brier")
    GAUGE_EWMA_DRIFT = Gauge("uco_ewma_drift", "EWMA drift (ECE+Brier+Conformal)")
    GAUGE_OOD_SHIFT = Gauge("uco_ood_shift", "OOD feature shift percent")
else:
    GAUGE_ECE = GAUGE_BRIER = GAUGE_EWMA_DRIFT = GAUGE_OOD_SHIFT = None

# ---------------------------
# Utilities: seeds, manifest, checkpoint hashing
# ---------------------------
def set_seeds(seed: int = DEFAULT_SEED):
    random.seed(seed)
    np.random.seed(seed)
    if has_torch:
        torch.manual_seed(seed)
        try:
            torch.cuda.manual_seed_all(seed)
        except Exception:
            pass

def manifest_print():
    manifest = {
        "python": sys.version.replace("\n", " "),
        "pytorch_present": has_torch,
        "seeds": SEEDS,
        "config": CONFIG
    }
    logger.info("Repro manifest: %s", json.dumps(manifest))

def sha256_of_file(path: str) -> str:
    h = hashlib.sha256()
    with open(path, "rb") as f:
        for chunk in iter(lambda: f.read(8192), b""):
            h.update(chunk)
    return h.hexdigest()

def save_checkpoint(state: Dict[str, Any], fname: Optional[str] = None) -> Tuple[str,str]:
    ts = int(time.time())
    if fname is None:
        fname = os.path.join(CHECKPOINT_DIR, f"uco_ckpt_{ts}.json")
    with open(fname, "w") as f:
        json.dump(state, f, default=lambda o: str(o), indent=2)
    h = sha256_of_file(fname)
    logger.info("Saved checkpoint %s (sha256=%s)", fname, h)
    return fname, h

# ---------------------------
# Metrics: ECE, Brier, conformal, entropy
# ---------------------------
def softmax(logits: np.ndarray) -> np.ndarray:
    # logits: (N, C)
    z = logits - np.max(logits, axis=1, keepdims=True)
    e = np.exp(z)
    return e / np.sum(e, axis=1, keepdims=True)

def brier_score(probs: np.ndarray, labels: np.ndarray) -> float:
    # multi-class Brier: mean sum((p - y_onehot)^2) across samples
    n, c = probs.shape
    y = np.zeros_like(probs)
    y[np.arange(n), labels] = 1.0
    return float(np.mean(np.sum((probs - y) ** 2, axis=1)))

def expected_calibration_error(probs: np.ndarray, labels: np.ndarray, n_bins: int = 15) -> float:
    # scalar ECE
    confidences = np.max(probs, axis=1)
    predictions = np.argmax(probs, axis=1)
    accuracies = (predictions == labels).astype(float)
    bins = np.linspace(0.0, 1.0, n_bins + 1)
    ece = 0.0
    for i in range(n_bins):
        mask = (confidences > bins[i]) & (confidences <= bins[i+1])
        if np.any(mask):
            acc = np.mean(accuracies[mask])
            conf = np.mean(confidences[mask])
            ece += (np.sum(mask) / len(probs)) * abs(acc - conf)
    return float(ece)

def entropy_of_probs(probs: np.ndarray) -> np.ndarray:
    # returns (N,) entropies
    p = np.clip(probs, 1e-12, 1.0)
    return -np.sum(p * np.log(p), axis=1)

# ---------------------------
# EWMA drift state
# ---------------------------
@dataclass
class DriftState:
    prev_drift: float = 0.0
    history: List[float] = field(default_factory=list)
    hist_ece: List[float] = field(default_factory=list)
    hist_brier: List[float] = field(default_factory=list)
    hist_conformal: List[float] = field(default_factory=list)
    last_checkpoint: float = field(default_factory=lambda: time.time())

    def update(self, ece: float, brier: float, conformal: float, alpha: float):
        combined = ece + brier + (1.0 - conformal)
        new_drift = alpha * combined + (1.0 - alpha) * self.prev_drift
        self.prev_drift = new_drift
        self.history.append(new_drift)
        self.hist_ece.append(ece)
        self.hist_brier.append(brier)
        self.hist_conformal.append(conformal)

    def hist_var(self) -> float:
        if len(self.history) < 2:
            return float(np.var(self.history)) if self.history else 0.0
        return float(np.var(self.history))

# ---------------------------
# OOD feature shift
# ---------------------------
@dataclass
class FeatureStats:
    hist_mean: Optional[np.ndarray] = None
    hist_std: Optional[np.ndarray] = None
    count: int = 0

    def update_from_batch(self, features: np.ndarray):
        # features: (N, D)
        if self.hist_mean is None:
            self.hist_mean = np.mean(features, axis=0)
            self.hist_std = np.std(features, axis=0) + 1e-8
            self.count = features.shape[0]
        else:
            # incremental update (online mean/std approximate)
            batch_mean = np.mean(features, axis=0)
            batch_var = np.var(features, axis=0)
            n1 = self.count
            n2 = features.shape[0]
            new_count = n1 + n2
            new_mean = (self.hist_mean * n1 + batch_mean * n2) / new_count
            # pooled variance (approx)
            new_var = ((n1 * (self.hist_std ** 2 + (self.hist_mean - new_mean) ** 2)) +
                       (n2 * (batch_var + (batch_mean - new_mean) ** 2))) / new_count
            self.hist_mean = new_mean
            self.hist_std = np.sqrt(new_var) + 1e-8
            self.count = new_count

    def percent_shift(self, features: np.ndarray) -> float:
        # compute norm of deviation / hist_std * 100 averaged over features
        if self.hist_mean is None:
            return 0.0
        dev = np.linalg.norm(np.mean(features, axis=0) - self.hist_mean) / np.linalg.norm(self.hist_std)
        return float(dev * 100.0)

# ---------------------------
# Placeholder research modules (stubs)
# ---------------------------
class NUCFL_Module:
    """ Non-Uniform Calibration for Federated Learning - placeholder """
    def __init__(self):
        pass

    def continual_update(self, probs: np.ndarray, labels: np.ndarray):
        # Returns an importance weight vector per class approx
        C = probs.shape[1]
        # naive: weight inversely proportional to class freq
        freq = np.bincount(labels, minlength=C).astype(float) + 1.0
        weights = (1.0 / freq) / np.sum(1.0 / freq)
        return weights

class CKDF_Module:
    """ CKDF-V2 representation shift detector - placeholder """
    def __init__(self):
        self.shift_est = 0.0

    def compute_shift(self, features: np.ndarray) -> float:
        # stub: compute small random shift metric
        self.shift_est = float(np.clip(np.abs(np.mean(features)) % 1.0, 0.0, 1.0))
        return self.shift_est

class LATPC_Module:
    """ LATPC-like latent adversarial trainer placeholder """
    def __init__(self):
        pass

    def train_latent_adv(self, model, features, labels):
        # stub: pretend training and return robustness metric in [0,1]
        return float(min(1.0, 0.5 + 0.5 * (np.random.rand())))

class ARS_fDP_Module:
    """ ARS with f-DP multi-step placeholder """
    def __init__(self, eps=1.0):
        self.eps = eps

    def certify(self, model, data):
        # stub: return certified radius estimate
        return float(self.eps * 0.5)

# ---------------------------
# Calibrators (temp scaling + optional isotonic)
# ---------------------------
class TemperatureScaler:
    def __init__(self):
        self.temp = 1.0

    def fit(self, logits: np.ndarray, labels: np.ndarray, lr=0.01, epochs=200):
        # simple gradient descent on NLL wrt temperature
        T = max(1e-3, float(self.temp))
        N, C = logits.shape
        labels = labels.astype(int)
        for _ in range(epochs):
            probs = softmax(logits / T)
            # negative log-likelihood
            nll = -np.mean(np.log(np.clip(probs[np.arange(N), labels], 1e-12, 1.0)))
            # gradient approx via finite diff
            eps = 1e-3
            probs_eps = softmax(logits / (T + eps))
            nll_eps = -np.mean(np.log(np.clip(probs_eps[np.arange(N), labels], 1e-12, 1.0)))
            grad = (nll_eps - nll) / eps
            T = max(1e-3, T - lr * grad)
        self.temp = T
        return self

    def transform(self, logits: np.ndarray) -> np.ndarray:
        return softmax(logits / float(self.temp))

class IsotonicCalibrator:
    def __init__(self):
        self.iso = None

    def fit(self, probs: np.ndarray, labels: np.ndarray):
        # fits per-class isotonic regressors on confidences -> accuracy
        # For brevity, we will fit a single isotonic on max-confidence -> correctness
        confidences = np.max(probs, axis=1)
        correctness = (np.argmax(probs, axis=1) == labels).astype(float)
        if has_isotonic:
            self.iso = IsotonicRegression(out_of_bounds='clip').fit(confidences, correctness)
        else:
            # fallback to identity mapping
            self.iso = None
        return self

    def transform(self, probs: np.ndarray) -> np.ndarray:
        if self.iso is None:
            return probs
        confs = np.max(probs, axis=1)
        mapped = self.iso.predict(confs)
        # scale the max-prob column to mapped, keep class proportions
        idx = np.argmax(probs, axis=1)
        adjusted = np.copy(probs)
        for i in range(len(probs)):
            c = idx[i]
            # set max to mapped[i], renormalize others proportionally
            others = np.delete(adjusted[i], c)
            others_sum = np.sum(others)
            if others_sum <= 0:
                adjusted[i] = 0
                adjusted[i, c] = mapped[i]
            else:
                scale = (1.0 - mapped[i]) / others_sum
                adjusted[i, c] = mapped[i]
                adjusted[i, np.arange(len(adjusted[i])) != c] = adjusted[i, np.arange(len(adjusted[i])) != c] * scale
        # avoid nan
        adjusted = np.nan_to_num(adjusted, nan=1.0/probs.shape[1])
        return adjusted

# ---------------------------
# Conformal (very simple CQR-like)
# ---------------------------
def conformal_interval_from_probs(probs: np.ndarray, labels: np.ndarray, alpha: float = 0.1):
    # Compute simple interval on predicted class probability for true label based on quantiles of residuals
    N = len(labels)
    pred_probs = probs[np.arange(N), labels]
    residuals = 1.0 - pred_probs
    q = np.quantile(residuals, 1.0 - alpha)
    lower = np.maximum(0.0, pred_probs - q)
    upper = np.minimum(1.0, pred_probs + q)
    return lower, upper, q

# ---------------------------
# Core: compute_ewma_drift & ood_feature_shift
# ---------------------------
def compute_ewma_drift(ece: float, brier: float, conformal: float, state: DriftState, alpha: float = None) -> float:
    if alpha is None:
        alpha = CONFIG["ewma_alpha"]
    state.update(ece=ece, brier=brier, conformal=conformal, alpha=alpha)
    return state.prev_drift

def ood_feature_shift(features: np.ndarray, feat_stats: FeatureStats) -> float:
    return feat_stats.percent_shift(features)

# ---------------------------
# Method selection & hysteresis
# ---------------------------
@dataclass
class HysteresisSelector:
    adapt_threshold_multiplier: float = 1.0
    window_history: List[bool] = field(default_factory=list)
    max_windows: int = 3

    def push(self, triggered: bool):
        self.window_history.append(triggered)
        if len(self.window_history) > self.max_windows:
            self.window_history.pop(0)

    def is_triggered(self) -> bool:
        return sum(self.window_history) >= (self.max_windows // 2) + 1

# ---------------------------
# Surrogate entropies & diagnostics
# ---------------------------
def surrogate_ent_batch_entropy(logits: np.ndarray) -> Tuple[float, float]:
    probs = softmax(logits)
    ent = entropy_of_probs(probs)
    return float(np.mean(ent)), float(np.std(ent))

# ---------------------------
# Retrain trigger
# ---------------------------
def trigger_retrain_condition(drift_state: DriftState, ood_shift_pct: float, diagnostics: Dict[str, float]) -> bool:
    # Conditions from spec
    ece_delta = diagnostics.get("delta_ece", 0.0)
    soft_ece = diagnostics.get("soft_ece", 0.0)
    brier = diagnostics.get("brier", 0.0)
    cal_conf = diagnostics.get("cal_conf", 1.0)  # calibration confidence
    conformal = diagnostics.get("conformal_q", 1.0)
    pac = diagnostics.get("pac", 0.0)
    # combine rules
    cond = False
    if ece_delta > CONFIG["drift_ece_delta_retrain"]:
        cond = True
    if ood_shift_pct > CONFIG["ood_shift_threshold_pct"]:
        cond = True
    if soft_ece > 0.05 and cal_conf < 0.5:
        cond = True
    # more conservative: require not in cooldown if last checkpoint recent
    now = time.time()
    last_ckpt = drift_state.last_checkpoint or 0.0
    if cond and (now - last_ckpt) < CONFIG["cooldown_seconds"]:
        logger.info("Retrain condition met but in cooldown (%.1f sec left)", CONFIG["cooldown_seconds"] - (now - last_ckpt))
        return False
    return cond

# ---------------------------
# Main calibrate_stream pipeline (single-batch)
# ---------------------------
def calibrate_stream(
    batch_logits: np.ndarray,
    batch_labels: np.ndarray,
    batch_features: np.ndarray,
    state: DriftState,
    feat_stats: FeatureStats,
    nucfl: NUCFL_Module,
    ckdf: CKDF_Module,
    latpc: LATPC_Module,
    ars: ARS_fDP_Module,
    selector: HysteresisSelector,
    min_samples_for_retrain: int = CONFIG["retrain_min_samples"]
) -> Tuple[np.ndarray, Callable[[np.ndarray], Tuple[np.ndarray, np.ndarray]], Dict[str, Any]]:
    """
    Returns:
      - calibrated_probs (N,C)
      - ci_func: function to compute (lower, upper) intervals for a new batch of probs
      - diagnostics dict
    """
    N = batch_logits.shape[0]
    # Base probs
    probs = softmax(batch_logits)
    # Basic metrics
    ece = expected_calibration_error(probs, batch_labels)
    brier = brier_score(probs, batch_labels)
    ent_mean, ent_std = surrogate_ent_batch_entropy(batch_logits)

    # Conformal intervals (train-time)
    lower, upper, q = conformal_interval_from_probs(probs, batch_labels, alpha=0.1)
    conformal_coverage = float(np.mean((batch_labels >= 0) * 1.0))  # placeholder

    # Compute EWMA drift
    drift = compute_ewma_drift(ece=ece, brier=brier, conformal=1.0 - q, state=state)

    # Feature stats update & OOD shift
    ood_shift_pct = ood_feature_shift(batch_features, feat_stats)
    feat_stats.update_from_batch(batch_features)

    # NUCFL continual update (weights)
    nucfl_weights = nucfl.continual_update(probs, batch_labels)

    # CKDF rep shift
    ckdf_shift = ckdf.compute_shift(batch_features)

    # LATPC training stub -> robustness metric
    latpc_robust = latpc.train_latent_adv(None, batch_features, batch_labels)

    # Surrogate uncertainties
    surrogate_ent, surrogate_unc = ent_mean, ent_std

    # Calibrator selection (hysteresis + adaptive threshold)
    # naive: start with TemperatureScaler; switch to Isotonic if ECE still high
    temp_scaler = TemperatureScaler()
    temp_scaler.fit(batch_logits, batch_labels, lr=0.01, epochs=100)
    calibrated_probs = temp_scaler.transform(batch_logits)

    iso_cal = IsotonicCalibrator()
    iso_cal.fit(calibrated_probs, batch_labels)
    calibrated_probs_iso = iso_cal.transform(calibrated_probs)

    # choose best among methods using ECE
    ece_temp = expected_calibration_error(calibrated_probs, batch_labels)
    ece_iso = expected_calibration_error(calibrated_probs_iso, batch_labels)
    if ece_iso < ece_temp:
        calibrator_used = "temperature+isotonic"
        final_probs = calibrated_probs_iso
    else:
        calibrator_used = "temperature"
        final_probs = calibrated_probs

    # Compute new metrics and deltas (delta vs last recorded hist if present)
    prev_ece = state.hist_ece[-1] if state.hist_ece else ece
    delta_ece = abs(ece - prev_ece)
    diagnostics = {
        "ece": ece,
        "brier": brier,
        "surrogate_ent_mean": surrogate_ent,
        "surrogate_ent_std": surrogate_unc,
        "ewma_drift": drift,
        "ood_shift_pct": ood_shift_pct,
        "nucfl_weights_sample": nucfl_weights.tolist() if hasattr(nucfl_weights, "tolist") else nucfl_weights,
        "ckdf_shift": ckdf_shift,
        "latpc_robust": latpc_robust,
        "calibrator_used": calibrator_used,
        "delta_ece": delta_ece,
        "conformal_q": float(q),
        "cal_conf": float(max(0.0, 1.0 - ece)),  # heuristic
        "pac": 0.0  # placeholder
    }

    # Prometheus push
    if GAUGE_ECE is not None:
        try:
            GAUGE_ECE.set(diagnostics["ece"])
            GAUGE_BRIER.set(diagnostics["brier"])
            GAUGE_EWMA_DRIFT.set(diagnostics["ewma_drift"])
            GAUGE_OOD_SHIFT.set(diagnostics["ood_shift_pct"])
        except Exception:
            pass

    # Hysteresis selection push
    retrain_flag = trigger_retrain_condition(state, ood_shift_pct, diagnostics)
    selector.push(retrain_flag)
    if selector.is_triggered() and state.hist_ece and len(state.hist_ece) * N >= min_samples_for_retrain:
        diagnostics["retrain_decision"] = True
    else:
        diagnostics["retrain_decision"] = False

    # update last checkpoint timestamp if checkpoint created elsewhere
    # Return CI function
    def ci_func(new_probs: np.ndarray, alpha: float = 0.1):
        # simple quantile-based interval per sample on top predicted class
        Nn = new_probs.shape[0]
        pred_conf = np.max(new_probs, axis=1)
        # widen interval using q from train-time
        lower = np.maximum(0.0, pred_conf - q)
        upper = np.minimum(1.0, pred_conf + q)
        return lower, upper

    return final_probs, ci_func, diagnostics

# ---------------------------
# Simulation harness (nightly synthetic adversarial + stress harness stub)
# ---------------------------
def synthetic_data_generator(N: int = 4096, C: int = 5, D: int = 32, seed: int = 42):
    rng = np.random.RandomState(seed)
    logits = rng.randn(N, C) * 1.0
    features = rng.randn(N, D)
    labels = rng.randint(0, C, size=(N,))
    return logits, labels, features

# ---------------------------
# Hook to perform "nightly synthetic adversarial" - stub
# ---------------------------
def nightly_synthetic_adversarial_run():
    logger.info("Running nightly synthetic adversarial harness (stub).")
    # produce synthetic dataset and compute some metrics
    logits, labels, features = synthetic_data_generator(N=12000, C=6, D=64, seed=random.choice(SEEDS))
    # pretend to run ARS f-DP and LATPC
    ars = ARS_fDP_Module(eps=CONFIG["ars_fdp_eps"])
    latpc = LATPC_Module()
    _ = ars.certify(None, None)
    _ = latpc.train_latent_adv(None, features, labels)
    logger.info("Nightly synthetic adversarial run complete.")

# ---------------------------
# Main entrypoint & CLI
# ---------------------------
def main(args):
    set_seeds(DEFAULT_SEED)
    manifest_print()
    if prometheus_available and args.prometheus_port:
        start_http_server(args.prometheus_port)
        logger.info("Prometheus metrics exposed at port %d", args.prometheus_port)

    # Initialize modules & state
    drift_state = DriftState()
    feat_stats = FeatureStats()
    nucfl = NUCFL_Module()
    ckdf = CKDF_Module()
    latpc = LATPC_Module()
    ars = ARS_fDP_Module(eps=CONFIG["ars_fdp_eps"])
    selector = HysteresisSelector(max_windows=CONFIG["hysteresis_windows"])

    # Option: run a simulation stream
    if args.simulate:
        logger.info("Starting simulated stream run (simulate mode).")
        total_samples = 0
        aggregated_diagnostics = []
        # simulate streaming in chunks
        for i in range(args.iterations):
            seed = random.choice(SEEDS)
            logits, labels, features = synthetic_data_generator(N=args.batch_size, C=args.num_classes, D=args.feature_dim, seed=seed + i)
            calibrated_probs, ci_fn, diagnostics = calibrate_stream(
                batch_logits=logits,
                batch_labels=labels,
                batch_features=features,
                state=drift_state,
                feat_stats=feat_stats,
                nucfl=nucfl,
                ckdf=ckdf,
                latpc=latpc,
                ars=ars,
                selector=selector,
                min_samples_for_retrain=CONFIG["min_N"]
            )
            total_samples += logits.shape[0]
            aggregated_diagnostics.append(diagnostics)
            logger.info("Iter %d | ece=%.6f brier=%.6f ewma=%.6f ood=%.3f cal=%s retrain=%s",
                        i, diagnostics["ece"], diagnostics["brier"], diagnostics["ewma_drift"],
                        diagnostics["ood_shift_pct"], diagnostics["calibrator_used"], diagnostics.get("retrain_decision", False))
            # periodic checkpoint
            if (time.time() - drift_state.last_checkpoint) > CONFIG["checkpoint_interval_sec"]:
                ckpt_state = {
                    "time": time.time(),
                    "diagnostics": diagnostics,
                    "drift_history_len": len(drift_state.history)
                }
                path, h = save_checkpoint(ckpt_state)
                drift_state.last_checkpoint = time.time()

        # Summary
        avg_ece = float(np.mean([d["ece"] for d in aggregated_diagnostics]))
        avg_brier = float(np.mean([d["brier"] for d in aggregated_diagnostics]))
        logger.info("Simulation complete. Samples=%d avg_ece=%.6f avg_brier=%.6f", total_samples, avg_ece, avg_brier)

    # Option: run nightly harness
    if args.nightly:
        nightly_synthetic_adversarial_run()

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="UCO V10.21 - Simulate / Run core pipeline")
    parser.add_argument("--simulate", action="store_true", help="Run simulated stream")
    parser.add_argument("--iterations", type=int, default=6, help="Number of stream iterations (batches)")
    parser.add_argument("--batch-size", type=int, default=4096, help="Batch size per iteration")
    parser.add_argument("--num-classes", type=int, default=5, help="Number of classes")
    parser.add_argument("--feature-dim", type=int, default=32, help="Feature dimensionality")
    parser.add_argument("--nightly", action="store_true", help="Run nightly synthetic harness")
    parser.add_argument("--prometheus-port", type=int, default=0, help="Expose prometheus metrics (port), 0=disabled")
    args = parser.parse_args()
    main(args)
